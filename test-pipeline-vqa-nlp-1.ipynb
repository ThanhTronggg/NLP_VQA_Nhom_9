{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11531814,"sourceType":"datasetVersion","datasetId":7042757},{"sourceId":11603566,"sourceType":"datasetVersion","datasetId":7277682},{"sourceId":11765061,"sourceType":"datasetVersion","datasetId":7385977},{"sourceId":236599851,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q underthesea\n!pip install -q rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T02:28:48.029478Z","iopub.execute_input":"2025-05-12T02:28:48.029772Z","iopub.status.idle":"2025-05-12T02:28:59.161071Z","shell.execute_reply.started":"2025-05-12T02:28:48.029741Z","shell.execute_reply":"2025-05-12T02:28:59.160309Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Create a VQA Model using CNN and LSTM","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torch.optim import AdamW\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\nimport seaborn as sns\nimport pandas as pd\nfrom transformers import AutoTokenizer, ViTFeatureExtractor, AutoModel, CLIPImageProcessor, AutoTokenizer, AutoImageProcessor, ViTModel\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom datasets import load_dataset\nfrom datasets import load_dataset, Features, Value\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport torch\nimport os\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom underthesea import word_tokenize, text_normalize\nimport re\nimport string\nfrom tqdm import tqdm\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport tensorflow as tf\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom sklearn.model_selection import train_test_split\nfrom rouge_score import rouge_scorer\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nTRAIN_BATCH_SIZE = 16\nVAL_BATCH_SIZE = 1\nMAX_LEN = 100\nEPOCHS = 2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T02:28:59.163259Z","iopub.execute_input":"2025-05-12T02:28:59.163607Z","iopub.status.idle":"2025-05-12T02:29:25.073676Z","shell.execute_reply.started":"2025-05-12T02:28:59.163583Z","shell.execute_reply":"2025-05-12T02:29:25.072883Z"}},"outputs":[{"name":"stderr","text":"2025-05-12 02:29:12.132611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747016952.317432      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747016952.370359      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Data visulization","metadata":{}},{"cell_type":"code","source":"root_dir = \"/kaggle/input/data-nlp-bai-2/data\"\ndata_1 = pd.read_csv(f\"{root_dir}/data_1.csv\")\ndata_2 = pd.read_csv(f\"{root_dir}/data_2.csv\")\ndata_3 = pd.read_csv(f\"{root_dir}/data_3.csv\")\ndata_4 = pd.read_csv(f\"{root_dir}/data_4.csv\")\ndata_5 = pd.read_csv(f\"{root_dir}/data_5.csv\")\ndata_6 = pd.read_csv(f\"{root_dir}/data_6.csv\")\ndata_7 = pd.read_csv(f\"{root_dir}/data_7.csv\")\ndata_8 = pd.read_csv(f\"{root_dir}/data_8.csv\")\ndata_9 = pd.read_csv(f\"{root_dir}/data_9.csv\")\ndf_1 = pd.concat([data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T02:29:25.074462Z","iopub.execute_input":"2025-05-12T02:29:25.075060Z","iopub.status.idle":"2025-05-12T02:29:35.198556Z","shell.execute_reply.started":"2025-05-12T02:29:25.075040Z","shell.execute_reply":"2025-05-12T02:29:35.197954Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"root_dir = '/kaggle/input/data-nlp-bai-2/img/img'\ndf = df_1.copy()\n# df = df.iloc[0:10000]\ndf = df.rename({\"image_path\": \"image_id\"}, axis = 1)\ndf[\"image_path\"] = df['image_id'].apply(lambda x: os.path.join(root_dir, x))\ndf[\"image_id\"] = df[\"image_id\"].str.split(\".\").str[0]\ndf = df.dropna(subset=['question','answer']).copy()\ndf['question'] = df['question'].astype(str)\ndf['answer'] = df['answer'].astype(str)\ndf.drop(['description'], axis = 1, inplace = True)\ndf.reset_index(inplace=True)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T02:29:35.199286Z","iopub.execute_input":"2025-05-12T02:29:35.199531Z","iopub.status.idle":"2025-05-12T02:29:36.449220Z","shell.execute_reply.started":"2025-05-12T02:29:35.199508Z","shell.execute_reply":"2025-05-12T02:29:36.448555Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"        index image_id                                           question  \\\n0           0        0          Cuốn sách trong ảnh thuộc bộ môn học nào?   \n1           1        0               Cuốn sách dành cho học sinh lớp nào?   \n2           2        0                     Cuốn sách được in lần thứ mấy?   \n3           3        0  Ai là người chủ biên phần tiếng Việt của cuốn ...   \n4           4        0                  Cuốn sách do đơn vị nào xuất bản?   \n...       ...      ...                                                ...   \n259089  19091    51855                    Câu hỏi 1 đề cập đến vấn đề gì?   \n259090  19092    51855  Câu hỏi 2 đề cập đến các biện pháp bảo vệ dữ l...   \n259091  19093    51855             Hình ảnh minh họa cho câu hỏi 1 là gì?   \n259092  19094    51855  Hình ảnh minh họa cho phần bài tập vận dụng là...   \n259093  19095    51855    Phần bài tập vận dụng yêu cầu người đọc làm gì?   \n\n                                                   answer  \\\n0               Cuốn sách trong ảnh thuộc bộ môn Ngữ văn.   \n1                     Cuốn sách dành cho học sinh lớp 12.   \n2                           Cuốn sách được in lần thứ ba.   \n3       Bùi Minh Toàn là người chủ biên phần tiếng Việ...   \n4       Cuốn sách được xuất bản bởi Nhà xuất bản Giáo ...   \n...                                                   ...   \n259089  Câu hỏi 1 đề cập đến khả năng lưu trữ văn bản ...   \n259090  Câu hỏi 2 đề cập đến 5 biện pháp bảo vệ dữ liệ...   \n259091  Hình ảnh minh họa cho câu hỏi 1 là 4 khung tho...   \n259092  Hình ảnh minh họa cho phần bài tập vận dụng là...   \n259093  Phần bài tập vận dụng yêu cầu người đọc nêu bi...   \n\n                                            image_path  \n0           /kaggle/input/data-nlp-bai-2/img/img/0.jpg  \n1           /kaggle/input/data-nlp-bai-2/img/img/0.jpg  \n2           /kaggle/input/data-nlp-bai-2/img/img/0.jpg  \n3           /kaggle/input/data-nlp-bai-2/img/img/0.jpg  \n4           /kaggle/input/data-nlp-bai-2/img/img/0.jpg  \n...                                                ...  \n259089  /kaggle/input/data-nlp-bai-2/img/img/51855.jpg  \n259090  /kaggle/input/data-nlp-bai-2/img/img/51855.jpg  \n259091  /kaggle/input/data-nlp-bai-2/img/img/51855.jpg  \n259092  /kaggle/input/data-nlp-bai-2/img/img/51855.jpg  \n259093  /kaggle/input/data-nlp-bai-2/img/img/51855.jpg  \n\n[259094 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>image_id</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>Cuốn sách trong ảnh thuộc bộ môn học nào?</td>\n      <td>Cuốn sách trong ảnh thuộc bộ môn Ngữ văn.</td>\n      <td>/kaggle/input/data-nlp-bai-2/img/img/0.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Cuốn sách dành cho học sinh lớp nào?</td>\n      <td>Cuốn sách dành cho học sinh lớp 12.</td>\n      <td>/kaggle/input/data-nlp-bai-2/img/img/0.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>Cuốn sách được in lần thứ mấy?</td>\n      <td>Cuốn sách được in lần thứ ba.</td>\n      <td>/kaggle/input/data-nlp-bai-2/img/img/0.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>Ai là người chủ biên phần tiếng Việt của cuốn ...</td>\n      <td>Bùi Minh Toàn là người chủ biên phần tiếng Việ...</td>\n      <td>/kaggle/input/data-nlp-bai-2/img/img/0.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>Cuốn sách do đơn vị nào xuất bản?</td>\n      <td>Cuốn sách được xuất bản bởi Nhà xuất bản Giáo ...</td>\n      <td>/kaggle/input/data-nlp-bai-2/img/img/0.jpg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>259089</th>\n      <td>19091</td>\n      <td>51855</td>\n      <td>Câu hỏi 1 đề cập đến vấn đề gì?</td>\n      <td>Câu hỏi 1 đề cập đến khả năng lưu trữ văn bản ...</td>\n      <td>/kaggle/input/data-nlp-bai-2/img/img/51855.jpg</td>\n    </tr>\n    <tr>\n      <th>259090</th>\n      <td>19092</td>\n      <td>51855</td>\n      <td>Câu hỏi 2 đề cập đến các biện pháp bảo vệ dữ l...</td>\n      <td>Câu hỏi 2 đề cập đến 5 biện pháp bảo vệ dữ liệ...</td>\n      <td>/kaggle/input/data-nlp-bai-2/img/img/51855.jpg</td>\n    </tr>\n    <tr>\n      <th>259091</th>\n      <td>19093</td>\n      <td>51855</td>\n      <td>Hình ảnh minh họa cho câu hỏi 1 là gì?</td>\n      <td>Hình ảnh minh họa cho câu hỏi 1 là 4 khung tho...</td>\n      <td>/kaggle/input/data-nlp-bai-2/img/img/51855.jpg</td>\n    </tr>\n    <tr>\n      <th>259092</th>\n      <td>19094</td>\n      <td>51855</td>\n      <td>Hình ảnh minh họa cho phần bài tập vận dụng là...</td>\n      <td>Hình ảnh minh họa cho phần bài tập vận dụng là...</td>\n      <td>/kaggle/input/data-nlp-bai-2/img/img/51855.jpg</td>\n    </tr>\n    <tr>\n      <th>259093</th>\n      <td>19095</td>\n      <td>51855</td>\n      <td>Phần bài tập vận dụng yêu cầu người đọc làm gì?</td>\n      <td>Phần bài tập vận dụng yêu cầu người đọc nêu bi...</td>\n      <td>/kaggle/input/data-nlp-bai-2/img/img/51855.jpg</td>\n    </tr>\n  </tbody>\n</table>\n<p>259094 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df = df[df['image_path'] != '/kaggle/input/data-nlp-bai-2/img/img/6803.jpg']\ndf = df[df['image_path'] != '/kaggle/input/data-nlp-bai-2/img/img/6801.jpg']\ndf[df[\"image_id\"] == \"6803\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T02:29:36.450094Z","iopub.execute_input":"2025-05-12T02:29:36.450341Z","iopub.status.idle":"2025-05-12T02:29:36.593069Z","shell.execute_reply.started":"2025-05-12T02:29:36.450324Z","shell.execute_reply":"2025-05-12T02:29:36.592428Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [index, image_id, question, answer, image_path]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>image_id</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df[df[\"image_id\"] == \"6801\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T02:29:36.593773Z","iopub.execute_input":"2025-05-12T02:29:36.594013Z","iopub.status.idle":"2025-05-12T02:29:36.619297Z","shell.execute_reply.started":"2025-05-12T02:29:36.593995Z","shell.execute_reply":"2025-05-12T02:29:36.618733Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [index, image_id, question, answer, image_path]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>image_id</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def normalize_text(text):\n    clean_text = text.lower()\n\n    replacements = {\n        \"nxb\": \"nhà xuất bản\",\n    }\n\n    for old, new in replacements.items():\n        clean_text = clean_text.replace(old, new)\n\n    # Xóa ký tự đặc biệt VD (&nbsp;)\n    clean_text = re.sub(r'&\\S+;', ' ', clean_text)\n\n    # Xóa emoji\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"\n                               u\"\\U0001F300-\\U0001F5FF\"\n                               u\"\\U0001F680-\\U0001F6FF\"\n                               u\"\\U0001F1E0-\\U0001F1FF\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u'\\U00010000-\\U0010ffff'\n                               u\"\\u200d\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\u3030\"\n                               u\"\\ufe0f\"\n                               \"]+\", flags=re.UNICODE)\n    clean_text = re.sub(emoji_pattern, \" \", clean_text)\n\n    # Xóa ký tự đặc biệt không gõ được\n    special_chars = [\"“\", \"”\", \"…\", \"•\", \"–\", \"’\", \" ️\", \"✅\", \"✓\", \" ̂́ ̃ ̣̂ \", \"‘\", \"●\", \"‒\", \"➤\",\n                     \"★\", \"ღ\", \"✪\", \"‎\", \"➦\", \"×\", \"\u0001\", \"✿\", \"☆\", \"◤\", \"◕\", \"❁\", \"‿\",\n                     \"❀\", \"■\", \"█\", \"☛\", \"⑴⒪⑵⑵⑴⑺\", \"►\", \"°\", \"»\", \"ø\", \"➽\", \"\", \"✧\", \"✽\", \"*\",\n                     \"➫\", \"【\", \"】\", \"⇒\", \"卐\", \"♛\", \"±\", \"∞\", \"②\", \"⑥\", \"①\", \"⑦\", \"➋\", \"➊\", \"➌\",\n                     \"✓\", \"™\", \"®\", \"\", \"\"]\n    for char in special_chars:\n        clean_text = clean_text.replace(char, \" \")\n\n    # Bỏ các ký tự đặc biệt (dấu câu)\n    clean_text = ''.join(' ' if char in string.punctuation else char for char in clean_text)\n\n    # Loại bỏ khoảng trắng thừa\n    clean_text = re.sub(r\"\\s+\", \" \", clean_text)\n    clean_text = re.sub(r\"^[\\s]\", \"\", clean_text)\n    clean_text = re.sub(r\"[\\s]$\", \"\", clean_text)\n\n    # Đảm bảo dấu ở đúng chữ (ví dụ: oà, uý)\n    clean_text = text_normalize(clean_text)\n\n    # Tách câu thành từ\n    clean_text = word_tokenize(clean_text, format=\"text\")\n\n    return clean_text\n\ndef apply_normalize_text_to_dataframe(df, columns):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: normalize_text(str(x)))\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T02:29:36.621774Z","iopub.execute_input":"2025-05-12T02:29:36.622260Z","iopub.status.idle":"2025-05-12T02:29:36.631589Z","shell.execute_reply.started":"2025-05-12T02:29:36.622242Z","shell.execute_reply":"2025-05-12T02:29:36.630968Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"columns_to_process = ['question', 'answer']\ndf = apply_normalize_text_to_dataframe(df, columns_to_process)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T02:29:36.632289Z","iopub.execute_input":"2025-05-12T02:29:36.632621Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data visualization","metadata":{}},{"cell_type":"code","source":"samples = df.sample(n=random.randint(2, min(2, len(df)))).reset_index(drop=True)\n\nfor i, row in samples.iterrows():\n    image = row['image_path']\n    question = row['question']\n    answer = row['answer']\n\n    try:\n        # Mở và hiển thị hình ảnh\n        img = Image.open(image)\n\n        plt.figure(figsize=(10, 10))\n        plt.imshow(img)\n        plt.title(f\"Câu hỏi: {question}\\nCâu trả lời: {answer}\", fontsize=10)\n        plt.axis('off')  # Ẩn trục để hiển thị gọn gàng hơn\n        plt.show()\n    except FileNotFoundError:\n        print(f\"Không tìm thấy hình ảnh: {image}\")\n    except Exception as e:\n        print(f\"Lỗi khi tải hình ảnh {image}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_sentence_length(text):\n    if not isinstance(text, str):\n        return 0\n    tokens = word_tokenize(text)\n    return len(tokens)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question_lengths = df['question'].apply(get_sentence_length).tolist()\nanswer_lengths = df['answer'].apply(get_sentence_length).tolist()\n\n# Thiết lập bố cục subplot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Biểu đồ 1: Độ dài câu hỏi\nax1.hist(question_lengths, bins=20, color='skyblue', edgecolor='black')\nax1.set_title('Phân bố độ dài câu hỏi')\nax1.set_xlabel('Số từ')\nax1.set_ylabel('Số lượng câu')\nax1.grid(True, alpha=0.3)\n\n# Biểu đồ 2: Độ dài câu trả lời\nax2.hist(answer_lengths, bins=20, color='lightgreen', edgecolor='black')\nax2.set_title('Phân bố độ dài câu trả lời')\nax2.set_xlabel('Số từ')\nax2.grid(True, alpha=0.3)\n\n# Tùy chỉnh khoảng cách giữa các subplot\nplt.tight_layout()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(text):\n    if not isinstance(text, str):\n        return \"\"\n    return word_tokenize(text, format=\"text\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tính độ tương đồng cosine\n# Chuẩn hóa văn bản cho TF-IDF\nquestions_preprocessed = df['question'].apply(preprocess_text).tolist()\nanswers_preprocessed = df['answer'].apply(preprocess_text).tolist()\n\n# Kết hợp câu hỏi và câu trả lời để tạo TF-IDF vector\ncorpus = questions_preprocessed + answers_preprocessed\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(corpus)\n\n# Tách TF-IDF matrix thành phần cho câu hỏi và câu trả lời\nquestion_tfidf = tfidf_matrix[:len(questions_preprocessed)]\nanswer_tfidf = tfidf_matrix[len(questions_preprocessed):]\n\n# Tính cosine similarity cho từng cặp câu hỏi-câu trả lời\nsimilarities = [cosine_similarity(question_tfidf[i], answer_tfidf[i])[0][0] for i in range(len(questions_preprocessed))]\n\n# Vẽ biểu đồ histogram cho độ tương đồng\nplt.figure(figsize=(8, 5))\nplt.hist(similarities, bins=20, color='lightcoral', edgecolor='black')\nplt.title('Phân bố độ tương đồng câu hỏi - câu trả lời')\nplt.xlabel('Cosine Similarity')\nplt.ylabel('Số lượng cặp')\nplt.grid(True, alpha=0.3)\n\n# Tùy chỉnh bố cục\nplt.tight_layout()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Add data gen","metadata":{}},{"cell_type":"code","source":"root_dir = \"/kaggle/input/data-generative/Data_genarative\"\ndf_list = []\n\nfor file_name in os.listdir(root_dir):\n    if file_name.endswith('.csv'):\n        file_path = os.path.join(root_dir, file_name)\n        try:\n            df_gen = pd.read_csv(file_path)\n            df_list.append(df_gen)\n            print(f\"Đã đọc file: {file_name}\")\n        except Exception as e:\n            print(f\"Lỗi khi đọc file {file_name}: {e}\")\n\ndf_gen1 = pd.concat(df_list, ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = '/kaggle/input/data-nlp-bai-2/img/img'\ndf_gen1 = df_gen1.rename({\"image_path\": \"image_id\"}, axis = 1)\ndf_gen1[\"image_path\"] = df_gen1['image_id'].apply(lambda x: os.path.join(root_dir, x))\ndf_gen1[\"image_id\"] = df_gen1[\"image_id\"].str.split(\".\").str[0]\ndf_gen1 = df_gen1.dropna(subset=['question','answer']).copy()\ndf_gen1['question'] = df_gen1['question'].astype(str)\ndf_gen1['answer'] = df_gen1['answer'].astype(str)\ndf_gen1.drop(['description'], axis = 1, inplace = True)\ndf_gen1.reset_index(inplace=True)\ndf_gen1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns_to_process = ['question', 'answer']\ndf_gen1 = apply_normalize_text_to_dataframe(df_gen1, columns_to_process)\ndf_gen1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_1 = df.copy()\ndf_gen_2 = df_gen1.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.concat([df_1, df_gen_2], ignore_index=True)\n\ndf = df.sort_values(by=['image_path', 'answer'], ignore_index=True)\ndf.drop_duplicates(inplace=True, ignore_index=True)\ndf.drop([\"index\"], axis = 1, inplace = True)\ndf.reset_index(inplace=True)\nnew_columns = [\"index\",'image_id', \"image_path\", 'question', 'answer']\ndf = df[new_columns]\ndf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question_lengths = df['question'].apply(get_sentence_length).tolist()\nanswer_lengths = df['answer'].apply(get_sentence_length).tolist()\n\n# Thiết lập bố cục subplot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Biểu đồ 1: Độ dài câu hỏi\nax1.hist(question_lengths, bins=20, color='skyblue', edgecolor='black')\nax1.set_title('Phân bố độ dài câu hỏi')\nax1.set_xlabel('Số từ')\nax1.set_ylabel('Số lượng câu')\nax1.grid(True, alpha=0.3)\n\n# Biểu đồ 2: Độ dài câu trả lời\nax2.hist(answer_lengths, bins=20, color='lightgreen', edgecolor='black')\nax2.set_title('Phân bố độ dài câu trả lời')\nax2.set_xlabel('Số từ')\nax2.grid(True, alpha=0.3)\n\n# Tùy chỉnh khoảng cách giữa các subplot\nplt.tight_layout()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(text):\n    # Chuyển thành chữ thường\n    text = text.lower()\n    # Loại bỏ dấu câu\n    text = re.sub(f'[{string.punctuation}]', ' ', text)\n    # Thay dấu '_' bằng khoảng trắng\n    text = text.replace('_', ' ')\n    # Tokenize bằng underthesea\n    text = word_tokenize(text, format=\"text\")\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_similarity(questions):\n    if len(questions) < 2:\n        return []  # Không tính nếu chỉ có 1 câu hỏi\n    \n    # Tiền xử lý các câu hỏi\n    processed_questions = [preprocess_text(q) for q in questions]\n    \n    # Vector hóa bằng TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(processed_questions)\n    \n    # Tính độ tương đồng cosine\n    similarity_matrix = cosine_similarity(tfidf_matrix)\n    \n    # Lưu kết quả cho từng cặp câu hỏi\n    similarities = []\n    for i in range(len(questions)):\n        for j in range(i + 1, len(questions)):\n            similarities.append({\n                'question_1': questions[i],\n                'question_2': questions[j],\n                'similarity': similarity_matrix[i][j]\n            })\n    \n    return similarities","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grouped = df.groupby('answer')\n\n# Lưu kết quả độ tương đồng\nall_similarities = []\n\n# Duyệt qua từng nhóm có cùng 'answer'\nfor answer, group in grouped:\n    if len(group) > 1:  # Chỉ xét các nhóm có từ 2 câu hỏi trở lên\n        questions = group['question'].tolist()\n        similarities = calculate_similarity(questions)\n        for sim in similarities:\n            sim['answer'] = answer  # Gắn câu trả lời vào kết quả\n            all_similarities.append(sim)\n\n# Chuyển kết quả thành DataFrame\nsimilarity_df = pd.DataFrame(all_similarities)\n\n# Sắp xếp theo độ tương đồng (giảm dần)\n# similarity_df = similarity_df.sort_values(by='similarity', ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"similarity_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(similarity_df['similarity'], bins=20, kde=True)\nplt.title('Phân bố độ tương đồng giữa các cặp câu hỏi')\nplt.xlabel('Độ tương đồng (Cosine Similarity)')\nplt.ylabel('Số lượng cặp câu hỏi')\nplt.grid(True)\nplt.savefig('similarity_histogram.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_group = grouped.size().idxmax()\ngroup = grouped.get_group(max_group)\nquestions = group['question'].tolist()\n\nif len(questions) > 1:\n    processed_questions = [preprocess_text(q) for q in questions]\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(processed_questions)\n    similarity_matrix = cosine_similarity(tfidf_matrix)\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(similarity_matrix, annot=True, cmap='Blues', xticklabels=questions, yticklabels=questions)\n    plt.title(f'Ma trận độ tương đồng cho nhóm có answer: \"{max_group}\"')\n    plt.xlabel('Câu hỏi')\n    plt.ylabel('Câu hỏi')\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.savefig('similarity_heatmap.png')\n    plt.show()\nelse:\n    print(f\"Nhóm có answer '{max_group}' chỉ có 1 câu hỏi, không vẽ heatmap.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenize\n","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\nvocab = tokenizer.get_vocab()\nlen(vocab)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train test split","metadata":{}},{"cell_type":"code","source":"# Chia dữ liệu thành tập huấn luyện và kiểm tra\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\n# Kiểm tra kích thước\nprint(\"Number of rows in train set:\", len(df_train))\nprint(\"Number of rows in test set:\", len(df_test))\nprint(\"Train set columns:\", df_train.columns)\nprint(\"Test set columns:\", df_test.columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Change form to dataset","metadata":{}},{"cell_type":"code","source":"class VQA_dataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.data = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        anno_id, img_id, image_path, question, answer = self.data.iloc[idx]\n        image = Image.open(image_path).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        return anno_id, img_id, image, question, answer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n\n    #Thêm 1 vài bước xử lý ảnh để đa dạng hóa dữ liệu - Data Augmentation\n    transforms.RandomRotation(30),  # Xoay ảnh ngẫu nhiên trong khoảng ±30 độ\n    # transforms.RandomHorizontalFlip(),  # Lật ngang ngẫu nhiên\n    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Điều chỉnh độ sáng, độ tương phản\n    # transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    \n    transforms.ToTensor(),\n    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n])\n\ntrain_dataset = VQA_dataset(df, transform=transform)\ntest_dataset = VQA_dataset(df_test, transform=transform)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = np.random.choice(len(train_dataset), 2)\n\nfor idx in random_indices:\n    anno_id, img_id, image, question, answer = train_dataset[idx]\n    \n    image = image.permute(1, 2, 0).numpy()\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(image)\n    plt.title(\"Question: \" + question + \"\\nAnswer: \" + answer)\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False)\nlen(train_loader), len(test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"anno_id, img_id, image, quesiton, answer = next(enumerate(train_loader))[1]\nlen(answer), len(image), len(quesiton)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Using Pretrain for VQA","metadata":{}},{"cell_type":"code","source":"image_model_name = \"google/vit-base-patch16-224-in21k\"\nclass ImageEmbedding(nn.Module):\n    def __init__(self):\n        super(ImageEmbedding, self).__init__()\n        self.process = AutoImageProcessor.from_pretrained(image_model_name)\n        self.model = ViTModel.from_pretrained(image_model_name)\n        #self.model = nn.Sequential(*list(self.model.children())[:3])\n        \n        for param in self.model.parameters():\n            param.requires_grad = False\n\n    def forward(self, image, image_ids):\n        inputs = self.process(image, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = self.model(**inputs.to(device))\n            \n        image_embedding = outputs.last_hidden_state\n        return image_embedding, image_ids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_model_name = \"vinai/phobert-base\"\nclass QuesEmbedding(nn.Module):\n    def __init__(self, input_size=768, output_size=768):\n        super(QuesEmbedding, self).__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n        self.phobert = AutoModel.from_pretrained(text_model_name)\n        self.lstm = nn.LSTM(input_size, output_size, batch_first=True)\n\n    def forward(self, ques):\n        tokenized_input = self.tokenizer(ques, \n                                         return_tensors='pt', \n                                         padding='max_length', \n                                         max_length=MAX_LEN, \n                                         truncation=True, \n                                         add_special_tokens=False)\n        ques = self.phobert(**tokenized_input.to(device)).last_hidden_state\n        _, (h, _) = self.lstm(ques)\n        return h.squeeze(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test Batch","metadata":{}},{"cell_type":"code","source":"image_model = ImageEmbedding().to(device)\nques_model = QuesEmbedding().to(device)\n\nfor batch in train_loader:\n    anno_ids, img_ids, images, questions, answers = batch\n    if torch.cuda.is_available():\n        images = images.cuda()\n        questions = questions\n        anno_ids = anno_ids\n    \n    with torch.no_grad():\n        image_embeddings, att_ids = image_model(images, image_ids=anno_ids)\n        ques_embeddings = ques_model(questions)\n    break    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(image_embeddings.size())\nprint(att_ids.size())\nprint(ques_embeddings.size())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_embeddings = image_embeddings.reshape(TRAIN_BATCH_SIZE, 768, -1).permute(0, 2, 1)\nques_embeddings = ques_embeddings.unsqueeze(1)\nprint(image_embeddings.size())\nprint(ques_embeddings.size())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stack Attention Networks","metadata":{}},{"cell_type":"code","source":"#Linear để giảm kích thước, không cần tile repeat mà đưa vào san để nhân luôn cuối cùng trả về vector attention mà ko cần phải flatten\nclass StackAttention(nn.Module):\n    def __init__(self, d=768, k=512, dropout=True):\n        super(StackAttention, self).__init__()\n        self.ff_image = nn.Linear(d, k)\n        self.ff_ques = nn.Linear(d, k)\n        if dropout:\n            self.dropout = nn.Dropout(p=0.5)\n        self.ff_attention = nn.Linear(k, 1)\n\n    def forward(self, vi, vq):\n        # N * 49 * 768 -> N * 49 * 512\n        hi = self.ff_image(vi)\n        #print(\"hi size: \", hi.size())\n        # N * 768 -> N * 512 -> N * 1 * 512\n        hq = self.ff_ques(vq)\n        #print(\"hq size: \", hq.size())\n        # N * 49 * 512\n        ha = F.tanh(hi + hq)\n        #print(\"ha1 size: \", ha.size())\n        if getattr(self, 'dropout'):\n            ha = self.dropout(ha)\n        #DONE\n        \n        # N * 49 * 512 -> N * 49 * 1 -> N * 49\n        ha = self.ff_attention(ha).squeeze(dim=2)\n        #print(\"ha2 size: \", ha.size())\n        pi = F.softmax(ha)\n        # (N * 49 * 1, N * 49 * 768) -> N * 768\n        vi_attended = (pi.unsqueeze(dim=2) * vi).sum(dim=1)\n        #print(\"vi~ size: \", vi_attended.size())\n        #print(\"vq size: \", vq.size())\n        u = vi_attended + vq.squeeze(1)\n        return u","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"san_model = StackAttention(d=768, k=512, dropout=True).to(device)\nimg_text_att = san_model(image_embeddings.to(device), ques_embeddings.to(device))\nimg_text_att.size()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate Answer","metadata":{}},{"cell_type":"markdown","source":"## Answers Embedding","metadata":{}},{"cell_type":"code","source":"class AnsEmbedding(nn.Module):\n    def __init__(self, input_size=768):\n        super(AnsEmbedding, self).__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n        self.phobert_embed = AutoModel.from_pretrained(text_model_name).embeddings.to(device)\n\n    def forward(self, ans):  \n        tokenized_input = self.tokenizer(ans, \n                                         return_tensors='pt',\n                                         padding='max_length', \n                                         max_length=MAX_LEN, \n                                         truncation=True, return_attention_mask=False)\n        ans = self.phobert_embed(**tokenized_input.to(device))\n        return tokenized_input['input_ids'], ans","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ans_model = AnsEmbedding().to(device)\n\nfor batch in train_loader:\n    anno_id, img_id, images, questions, answers = batch\n    if torch.cuda.is_available():\n        answers = answers\n    with torch.no_grad():\n        ans_tokens, answers_embedding = ans_model(answers)\n    break   \nans_tokens.size(), answers_embedding.size() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(answers_embedding.size())\nprint(img_text_att.size())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Decoder","metadata":{}},{"cell_type":"code","source":"def scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size()[-1] \n    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k) \n    if mask is not None:\n        scaled += mask \n    attention = F.softmax(scaled, dim=-1) \n    values = torch.matmul(attention, v)\n    return values, attention\n\n\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, hidden, drop_prob=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, hidden)\n        self.linear2 = nn.Linear(hidden, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.dropout(x) \n        x = self.linear2(x)\n        return x\n\n\nclass LayerNormalization(nn.Module):\n    def __init__(self, parameters_shape, eps=1e-5):\n        super().__init__()\n        self.parameters_shape=parameters_shape\n        self.eps=eps\n        self.gamma = nn.Parameter(torch.ones(parameters_shape)) # 512\n        self.beta =  nn.Parameter(torch.zeros(parameters_shape)) # 512\n\n    def forward(self, inputs):\n        dims = [-(i + 1) for i in range(len(self.parameters_shape))] # [-1]\n        mean = inputs.mean(dim=dims, keepdim=True) \n        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n        std = (var + self.eps).sqrt() \n        y = (inputs - mean) / std \n        out = self.gamma * y  + self.beta  \n        return out\n\nclass MaskMultiHeadAttention(nn.Module):\n\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.qkv_layer = nn.Linear(d_model , 3 * d_model) \n        self.linear_layer = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, mask):\n        batch_size, sequence_length, d_model = x.size() \n        qkv = self.qkv_layer(x) \n        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3) \n        q, k, v = qkv.chunk(3, dim=-1) \n        values, attention = scaled_dot_product(q, k, v, mask) \n        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim) \n        out = self.linear_layer(values) # 30 x 200 x 512\n        return out # 30 x 200 x 512\n\n\nclass MultiHeadCrossAttention(nn.Module):\n\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.kv_layer = nn.Linear(d_model , 2 * d_model) # 1024\n        self.q_layer = nn.Linear(d_model , d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, y, mask=None):\n        batch_size, sequence_length, d_model = x.size()\n        kv = self.kv_layer(x) \n        q = self.q_layer(y) \n        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim) \n        kv = kv.permute(0, 2, 1, 3) \n        q = q.permute(0, 2, 1, 3) \n        k, v = kv.chunk(2, dim=-1) \n        values, attention = scaled_dot_product(q, k, v, mask) \n        values = values.reshape(batch_size, sequence_length, d_model) \n        out = self.linear_layer(values)\n        return out  \n\n\nclass DecoderLayer(nn.Module):\n\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(DecoderLayer, self).__init__()\n        self.self_attention = MaskMultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm3 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout3 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, y, decoder_mask):\n        _y = y\n        y = self.self_attention(y, mask=decoder_mask)\n        y = self.dropout1(y)\n        y = self.norm1(y + _y)\n\n        _y = y # 30 x 200 x 512\n        y = self.encoder_decoder_attention(x, y, mask=None)\n        y = self.dropout2(y)\n        y = self.norm2(y + _y)\n\n        _y = y \n        y = self.ffn(y) \n        y = self.dropout3(y)\n        y = self.norm3(y + _y) \n        return y \n\nclass SequentialDecoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, y, mask = inputs\n        for module in self._modules.values():\n            y = module(x, y, mask)\n        return y\n\nclass Decoder(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers=1):\n        super().__init__()\n        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) \n                                          for _ in range(num_layers)])\n\n    def forward(self, x, y, mask):\n        y = self.layers(x, y, mask)\n        return y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"d_model = 768\nnum_heads = 8\ndrop_prob = 0.1\nbatch_size = 30\nmax_sequence_length = MAX_LEN\nffn_hidden = 2048\nnum_layers = 5\n\nx = img_text_att.unsqueeze(1).expand(-1, max_sequence_length, -1).to(device) # 16 * 768 -> 16 * 48 * 1024\ny = answers_embedding.to(device) #16 * 48 * 768\nmask = torch.full([max_sequence_length, max_sequence_length] , float('-inf'))\nmask = torch.triu(mask, diagonal=1).to(device)\ndecoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers).to(device)\nout = decoder(x, y, mask).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size = len(vocab)\n\nlinear_layer = nn.Linear(d_model, vocab_size).to(device)\noutput_logits = linear_layer(out)\noutput_probs = F.softmax(output_logits, dim=2)\noutput_probs.size()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import PhobertTokenizer\ntokenizer = PhobertTokenizer.from_pretrained(\"vinai/phobert-base\")\n\ndecoder_output = output_probs\npredicted_tokens = decoder_output.argmax(dim=2)\n\ndef tokens_to_text(tokens, tokenizeFcr):\n    return [tokenizer.decode(token.item()) for token in tokens]\n\nfor i in range(16):\n    generated_text = tokens_to_text(predicted_tokens[i], tokenizer)\n    print(f\"Generated text for example {i}: {' '.join(generated_text)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VQA Model","metadata":{}},{"cell_type":"code","source":"class VQAModel(nn.Module):\n\n    def __init__(self, vocab_size=64001, output_size=768, d_model=768, num_head=8, ffn_hidden=2048, drop_prob=0.3, num_layers=6, num_att_layers=4, mode='train'):\n        super(VQAModel, self).__init__()\n        self.mode = mode\n        self.image_model = ImageEmbedding().to(device)\n        self.ques_model = QuesEmbedding().to(device)\n        self.ans_model = AnsEmbedding().to(device)\n        \n        self.san_model = nn.ModuleList(\n            [StackAttention(d=d_model, k=512, dropout=True)] * num_att_layers).to(device)\n        \n        self.tanh = nn.Tanh()\n        self.dropout = nn.Dropout(0.5)\n        \n        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers).to(device)\n        \n        self.mlp = nn.Sequential(\n            nn.Dropout(p=0.3),\n            nn.Linear(d_model, vocab_size))\n\n    def forward(self, images, questions, answers, anno_ids, mask, mode, max_len=MAX_LEN):\n        image_embeddings, att_ids = self.image_model(images.to(device), image_ids=anno_ids)\n        if mode == 'train':\n            image_embedds = image_embeddings.reshape(TRAIN_BATCH_SIZE, 768, -1).permute(0, 2, 1)\n        else:\n            image_embedds = image_embeddings.reshape(VAL_BATCH_SIZE, 768, -1).permute(0, 2, 1)\n        \n        ques_embeddings = self.ques_model(questions)\n        ques_embedds = ques_embeddings.unsqueeze(1)\n        \n        for att_layer in self.san_model:\n            att_embedds = att_layer(image_embedds.to(device), ques_embedds.to(device))\n        \n        att_embedds = self.tanh(att_embedds)\n        att_embedds = self.dropout(att_embedds)\n        \n        #START DECODER\n        ans_vocab, ans_embedds = self.ans_model(answers)\n        \n        x = att_embedds.to(device).unsqueeze(1).expand(-1, max_len, -1).to(device) # 16 * 768 -> 16 * 48 * 768\n        y = ans_embedds # 16 * 48 * 768\n        #print(x)\n        #print(y.size())\n        if mask == False:\n            out = self.decoder(x, y, mask=None).to(device)\n        else:\n            mask = torch.full([max_len, max_len] , float('-inf'))\n            mask = torch.triu(mask, diagonal=1).to(device)\n        \n            out = self.decoder(x, y, mask).to(device)\n        #END DECODER\n        #print(out)\n        output_logits = self.mlp(out)\n        return output_logits, ans_vocab","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"model = VQAModel().to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index=1)\noptimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.001)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    # for parameter in model.parameters():\n    #     parameter.requires_grad = False\n    \n    # model.eval()\n    model.train()\n    \n    return model\n\nmodel = load_checkpoint('/kaggle/input/model-vqa-nlp/model_ViT_PhoBert.pt').to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'The model has {count_parameters(model):,} trainable parameters.')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_swap = {value: key for key, value in vocab.items()}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(model, test_loader, criterion, vocab_swap, device, print_every=500, max_len=MAX_LEN):\n    model.eval()  # Chuyển sang chế độ đánh giá\n    test_losses = []\n    total_loss = 0.0\n    \n    with torch.no_grad():  # Tắt tính toán gradient\n        for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n            anno_id, img_id, images, questions, answers = batch\n        \n            images = images.to(device)\n            \n            # Dự đoán\n            predicted_tokens, ans_embedds = model(images, questions, answers, anno_id, mode='test', mask=False)\n            predicted_tokens = predicted_tokens.float()\n            ans_embedds = ans_embedds.long()\n            \n            # Tính loss\n            loss = criterion(predicted_tokens.permute(0, 2, 1), ans_embedds)\n            total_loss += loss.item()\n            test_losses.append(loss.item())\n            \n            # In kết quả dự đoán và câu trả lời thực tế\n            sentence_predicted = torch.argmax(predicted_tokens[i], axis=1)\n            predicted_sentence = \"\"\n            for idx in sentence_predicted:\n                predicted_sentence += vocab_swap[idx.item()] + \" \"\n                if idx == 2:  # Dừng khi gặp token kết thúc\n                    break\n            \n            # In thông tin\n            print(f\"Batch [{batch_idx + 1}/{len(test_loader)}], Sample {i + 1}/{len(images)}\")\n            print(f\"Question: {questions[i]}\")\n            print(f\"Generated Answer: {predicted_sentence.strip()}\")\n            print(f\"Actual Answer: {answers[i]}\")\n            print(f\"Loss: {loss.item():.4f}\")\n            print(\"----------------------------\")\n    \n        # In loss trung bình\n        avg_loss = total_loss / len(test_loader)\n        print(f\"Average Test Loss: {avg_loss:.4f}\")\n    \n    return test_losses, avg_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_losses, avg_loss = evaluate(model, test_loader, criterion, vocab_swap, device, print_every=500, max_len=MAX_LEN)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}